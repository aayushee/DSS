%% 
%% Copyright 2007, 2008, 2009 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01

\documentclass[preprint,11pt]{elsarticle}

%% Use the option review to obtain double line spacing
 %\documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\usepackage{graphicx}
\graphicspath{ {images/}}
%\usepackage[boxed]{algorithm2e}
%\usepackage{algpseudocode}
\usepackage{slashbox}
\journal{Decision Support Systems}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{Evaluation of spell correction on noisy OCR data}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \address[label1]{}
%% \address[label2]{}

\author{Aayushee Gupta, Haimonti Dutta}

\address{}



\begin{abstract}
%% Text of abstract
Optical Character Recognition (OCR) of historical text often leads to several kinds of spelling errors. Existing spell correction algorithms do not present a rigorous performance evaluation of the spell correction process. In this paper, we present a novel N-gram based algorithm for evaluation of spell correction which can handle noisy and cleaned text of different lengths.
 %First, an edit distance based spell correction algorithm is presented which forms the basis of the evaluation mechanism. The goal of the Spell Correction Evaluation (SCE) algorithm is to measure accuracy of the text corrected by the spell correction algorithm against manually corrected data. N word grams are used to make word-to-word correspondences between the corrected text and the noisy OCR text - each token is then annotated as a True/False Positive/Negative of the correction process. 
The algorithm relies on appropriately choosing a window of N-words and aligning them in three parallel corpora - noisy OCR, corrected, and cleaned and annotated text (ground truth). Empirical results of spell correction on 300 news articles from the ``The Sun" newspaper, Nov- Dec 1894 are presented on edit distance and context sensitive based spell correctors. The Spell Correction Evaluation (SCE) algorithm evaluates their AUC values to be 0.49 and 0.56 respectively. We posit that this novel algorithm for spell correction evaluation has a wide applicability for comparing multiple spell correction algorithms which can play a crucial role in analyzing large volumes of digitized OCR text.
\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
 OCR \sep Spell Correction \sep Spell Correction Evaluation \sep Historical Newspaper Archives
%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text


\section{Introduction}

OCR of typed, handwritten or printed text is widely used to obtain digitized text which can be edited, searched, stored and displayed efficiently(\cite{torget2011mapping,palfray2012logical}). It is used in various applications such as banking, digital libraries \cite{mcmurdo2013vermont} and repositories, number plate recognition, and handwriting recognition \cite{singh2012survey}. However, the OCR scanning of printed text generates a lot of garbled text which renders them inadequate for any such tasks.
/////DOUBT: Should it be mentioned that OCR of specifically historical documents generates garbled text?

 Refinement of such noisy OCR text through spell correction can make them useful for text mining tasks (\cite{dutta2011learning}, \cite{yang2011topic}). 

//////////DOUBT: Can we say that the USP of our algorithm is that it can be easily used to compare amongst multiple spell correction algorithms and our algorithm will tell which one is suited to a user's application best?

Most spell correction algorithms have focused on improving the correction model and either do not give a detailed performance evaluation of the algorithm post spell correction or the evaluation measures used are not able to completely analyze the performance of such algorithms.  
A major problem that surfaces when evaluating a spell corrector for OCR text is that the text has to be verified against the original text (ground truth) to estimate its performance. This one-to-one verification may lead to word alignment problems, since the corrected and original text can be of different lengths because of the variety of errors in the OCR text.
In this paper, we describe the development of an N-word grams Spell Correction Evaluation (SCE) algorithm that can automatically evaluate a spell correction algorithm by using an N-word window to align three parallel corpora - the noisy OCR, corrected and original/ manually cleaned text. 
We present the performance evaluation results of applying our SCE algorithm on two  commonly used spell correction techniques - Edit distance and Context Sensitive spelliing correction on 300 OCR historical newspaper articles of ``The Sun" newspaper published in Nov-Dec 1894. 

\noindent \textbf{Organization: } This paper is organized as follows: related work is described in Section~\ref{spell:rw}; characteristics of the OCR data and its annotation in Section~\ref{spell:errors}; spelling correction and evaluation algorithms in Section~\ref{spell:algo}; empirical evaluation in Section~\ref{spell:results} followed by discussion and future work in Section~\ref{spell:fw}. 

\section{Related Work}
\label{spell:rw}
\input{RelatedWork}


\section{Data}
\label{spell:errors}

\subsection{Data Characteristics}

The dataset used for empirical evaluation of the algorithm has been obtained from the Chronicling America\footnote{\texttt{http://chroniclingamerica.loc.gov/}} website. It contains scanned newspaper pages published in New York between1890 to 1920. OCR software is run over high
resolution images to create searchable full text of the newspaper articles.

In order to make a newspaper available for searching on the Internet,
the following processes used in \cite{dutta2011learning} must take place: (1) the microfilm copy or
paper original is scanned; (2) master and Web image files are
generated; (3) metadata is assigned for each page to improve the
search capability of the newspaper; (4) OCR software is run over high
resolution images to create searchable full text and (5) OCR text,
images, and metadata are imported into a digital library software
program. 
The scanned newspaper holdings of the NYPL offers a wealth of
data and opinion for researchers and historians.The newspaper titles and digitized pages available through the
Chronicling America website can be searched using the OpenSearch
protocol\footnote{\texttt{http://www.opensearch.org/Home}}.
Unfortunately, the current search facilities are rudimentary and
irrelevant documents are often more highly ranked than relevant ones.
The newspapers are scanned on a page-by-page basis and article level
segmentation is poor or non-existent; the OCR scanning process is far
from perfect and the documents generated from it contains a large
amount of garbled text. In a bid to serve its patrons better, the New
York Public Library employed human annotators to clean headlines of
articles and text, but the process of manually reading all the old
newspapers article-by-article and cleaning them soon became very
expensive.




\begin{figure}[hbt]
\includegraphics[scale=0.75]{originalimage}
\includegraphics[scale=0.80]{ocr}
\caption{Scanned Image of a Newspaper article (left) and its OCR raw text (right)}
\label{figure:1}
\end{figure}

%\noindent An individual OCR text article has at least one or more of the following types of spelling errors: (1) \textbf{Real word errors: }include words that are spelled correctly in the OCR text but still incorrect when compared to the original newspaper article image. %Example\footnote{All the examples are illustrated in Figure~\ref{figure:1}}: ``coil"  has been correctly spelled in the OCR text  but should have been ``and" according to the original newspaper article. (2) \textbf{Non-real word errors: }include words that have been misspelled %due to some insertion, deletion, substitution or transposition of characters from a word. Example: ``tnenty" in the OCR text has a substitution error (`n' should have been `w') which is actually ``twenty" according to the original newspaper article. (3) \textbf{Non-word errors: %}include words that have been spelled incorrectly and are a combination of alphabets and numerical characters. Example:  ``4anrliteii" which is a combination of alphabets and number and should have been ``confident" as per the original newspaper article. (4) \textbf{New %Line errors: }include words that are separated by hyphens where part of a word is written on one text line and remaining part in the next line. Example: ``ex-ceptionally" where ``ex" occurs on one line while ``ceptionally" in the next and due to no punctuation in the text, %they are treated as separate words in OCR text. (5) \textbf{Word Split and Join errors: }include words that either get split into one of more parts or some words in a sentence get joined to a make a single word. Example: ``Thernndldntesnra" in the OCR text is actually a %combination of three words ``The candidates are" while the words ``v Icrory" are actually equivalent to a single word ``victory" when compared with the original news article.



 An individual OCR text article has at least one or more of the following types of spelling errors:
\begin{description}
 \item[$\bullet$Real word errors] include words that are spelled correctly in the OCR text but still incorrect when compared to the original newspaper article image. For example: In Figure~\ref{figure:1}, the word ``coil"  has been correctly spelled in the OCR text  but should have been ``and" according to the original newspaper article. 
 \item[$\bullet$Non-real word errors] include words that have been misspelled due to some insertion, deletion, substitution or transposition of characters from a word. For eg. In Figure~\ref{figure:1}, the word ``tnenty" in the OCR text has a substitution error (`n' should have been `w') which is actually ``twenty" according to the original newspaper article.
 \item[$\bullet$Non-word errors] include words that have been spelled incorrectly and are a combination of alphabets and numerical characters. For example: In Figure~\ref{figure:1}, the word ``4anrliteii" which is a combination of alphabets and number and should have been ``confident" as per the original newspaper article.
\item[$\bullet$New Line errors] include words that are separated by hyphens where part of a word is written on one text line and remaining part in the next line. For example: In Figure~\ref{figure:1}, the word ``ex-ceptionally" where ``ex" occurs on one line while ``ceptionally" in the next and due to no punctuation in the text, they are treated as separate words in OCR text.
\item[$\bullet$Word Split and Join errors] include words that either get split into one of more parts or some words in a sentence get joined to a make a single word. For example: In Figure~\ref{figure:1}, the word ``Thernndldntesnra" in the OCR text is actually a combination of three words ``The candidates are" while the words ``v Icrory" are actually equivalent to a single word ``victory" when compared with the original news article.
\end{description} 

\subsection{Data Statistics}
Our corpus consists of 300 OCR text articles from the ``The Sun" newspaper issues of November-December 1894 taken from the Chronicling America website. The text does not have any punctuation and contains a large amount of garbled text containing OCR errors mentioned in Section~\ref{spell:errors}.
 Spelling correction is performed on this corpus to obtain spell corrected text. We also used manually cleaned OCR text to check and validate the correctness of spelling correction procedure. 


%The OCR text available is on a page-by-page level and no article level segmentation is provided. OCR text dataset is therefore, taken from a PostgreSQL database where article level segmentation of page-level OCR text from Chronicling America is available for
Two months of articles of ``The Sun" newspaper from November-December 1894 consisting of 14020 news articles with a total of 8,403,844 tokens are used for empirical evaluation. %The newspaper database ER diagram \footnote{https://power.ldeo.columbia.edu/twiki/pub/Incubator/BodhiDBDesign/Final ERD.pdf }
%is used to extract the required articles text from the database by dumping complete dataset and extracting individual articles linetext based on their unique ID. 



 
Due to the word split and join errors and new line errors in the OCR text, the 3 parallel corpora , i.e., OCR text, its spell corrected version and the original correct version need to be aligned so that a word by word correspondence could be made among all versions and the spelling correction algorithm could be evaluated.

\subsection{Manual Annotation of Data}
For the evaluation of spelling correction on spell corrected data, cleaned/ original newspaper text is required so that a word by word matching can be done to see how well the spelling corrector performs. Due to unavailability of a spell corrected and original correct word pairs list, manual annotation of the OCR text articles needs to be done so that the spelling correction evaluation can be done accordingly. 
In order to obtain the original cleaned text from the OCR text, we used manual annotators who annotated the OCR articles from their newspaper images. For this purpose, we recruited 5 annotators each of whom were given 50 OCR articles and their corresponding images to annotate. For every OCR article, they had to write the corresponding clean line text called as the original newspaper article version by looking at the OCR article's newspaper image/s. The annotators were also asked not to consider any punctuation in their writing of clean text since the OCR text from Chronicling America is devoid of the same. 
DOUBT: Does this need to be mentioned here: The OCR text article was given as reference so that the number of linetexts in the correctly written version from newspaper image and the OCR text article remain same.
 Each annotator was given a different set of articles to work with ranging from November-December issue of ``The Sun" newspaper and after the annotation process was complete, one more annotator was asked to verify whether the resulting original article was completely error free or not.





\section{Theory}
\label{spell:algo}
%Several kinds of spelling errors can exist in the OCR text as described in the previous section. 
%
%
%% DOUBT: Do the following lines need to be removed?
%%The garbled OCR dataset  needs to be refined by correcting the text with the help of a human editor manually or automatic spelling corrector. Due to the huge size of dataset, human editing would be extremely time consuming and expensive making it impossible and indicating requirement of a spelling correction technique. 
%We use edit distance algorithm for spelling correction because of its speed and ability to correct OCR errors compared to the n-gram approach \cite{chattopadhyaya2013fast}. Context-dependent spelling correction is not used because of unavailability of n-gram words corpus or ground truth dataset containing OCR and true word pairs.
%
This section first describes the two types of algorithms- Edit Distance and Context Sensitive Spell Correction Algorithms followed by a detailed description of the N-Gram based Spell Correction Evaluation (SCE) Algorithm on which the first two algorithms are tested.

\subsection{ Edit Distance Spelling Corrector}
\label{spell:algorithm}
The Edit Distance algorithm based on Levenshtein distance\cite{levenshtein1966binary} has been used for spelling correction. It is an isolated word correction technique that uses dictionary based-look up and the distance between strings for matching the text and correcting it. An ``edit distance"\footnote{Our edit distance algorithm corrects non-real word spelling errors by making at most 2 operations of insertion, deletion and substitution of letters in the word. The choice of 2 is governed by the trade off between algorithm runtime and quality of spelling correction. The spelling corrector has been designed as suggested by Peter Norvig\texttt{ http://norvig.com/spell-correct.html}.} corresponds to the minimum number of insertions, deletions, and substitutions required to transform one string into another. %The algorithm requires a dictionary which is used to check if each word of the text exists in it or not. If it does, then no change is made to the word; otherwise, a candidate list of words is created from the word to be corrected by inserting, substituting or deleting up to 2 letters from it.  This list of words is returned as suggestions for the word to be corrected. The correction is made with the word formed from lowest edit distance and occurring with more frequency in the dictionary. This makes the edit distance algorithm dependent on the type of dictionary chosen for correction.
% which means the dictionary must be well chosen for spelling correction of a specific document collection. 
%The algorithm is made to run fast by reading the dictionary only once and keeping a data structure in memory for its word counts which can be referred to whenever a word comes up for correction.


\subsection{Context Sensitive Spelling Corrector}
We use LingPipe's spelling correction algorithm for spelling correction based on context. It is based on a noisy-channel model, which models user mistakes (typos and brainos) and expected user input (based on the data). Mistakes are modeled by weighted edit distances and expected input by a character language model.
Spelling corrections are motivated by the indexed dictionary which make the algorithm domain sensitive.  The dictionary words are added to the search index and a language model. The language model stores seen phrases and maintains statistics about them. When a word is submitted for correction, the spell corrector looks for possible character edits, namely substitutions, insertions, replacements, transpositions, and deletions, that make the word to be corrected a better fit for the language model. The spell corrector is also able to split and combine tokens during spelling correction due to which Word Split and Join errors can also be corrected by this algorithm.


\subsection{Spelling Correction Algorithm Evaluation }
\label{spell:sce}
For evaluating the performance of spell correction, the raw OCR text and OCR text after application of spelling correction algorithm (corrected text) needs to be compared with the original newspaper text. The OCR text is extremely garbled with Word Split and Join errors due to which word-to-word alignment with the original newspaper text is impossible,i.e., the raw OCR and original newspaper text are of different lengths. A novel algorithm,Spelling Correction Evaluation (SCE) based on N-gram approach is proposed for automatic evaluation of the corrected text. %word-by-word against the manually corrected subset of the news articles dataset. 
The SCE algorithm can be used for evaluation of any type of spelling correction algorithm - dictionary look up, context sensitive or probabilistic spell correction algorithms.
The  following metrics are used for estimating the performance:
%\noindent \textbf{Evaluation Parameters}
%\begin{description}
% \item[$\bullet$Accuracy]
(1) \textbf{Accuracy }This requires calculation of the number of OCR errors that got corrected when compared to the original newspaper text. Specifically,
%The measure has been chosen so as to include the complete text coverage and not just check for words that get corrected after spell correction as in the latter case, the  number of False Postivies and True Negatives get missed which does not give the correct measure of how well the spell corrector works. The formula used for calculating Accuracy is defined by Manning and Schutze,1999 (p.268-269) as follows:
$Accuracy=  \frac{TP+TN} {TP+ FP + TN + FN}$ where, $TP$=Number of True Positives, $TN$=Number of True Negatives, $FP$=Number of False Positives, $FN$=Number of False Negatives. %The aim of the SCE algorithm is to make a word-to-word correspondence between the OCR corrected text and the original newspaper text and to mark each token in the OCR text as a $TP$, $FP$, $TN$ or $FN$. 
Reynaert and Martin\cite{reynaert2008all} suggest a way to define these terms by distinguishing between correct words and incorrect words in the text through the set of non-target, target and selected words and use Precision and Recall evaluation measures for measuring performance of spelling correction which we adapt for this work. %According to our SCE algorithm, a ``true positive" is said to occur when a word from the OCR text gets corrected and the corrected spelling matches the one in original article text while a ``false positive" occurs if the corrected spelling does not match the corresponding word in the original article text. A ``true negative" occurs when a word does not get corrected by the algorithm as it is already correct and matches the correct word in the original text also. On the other hand, a ``false negative" occurs when the algorithm is unable to correct the word (there is no change in spelling of the word) and it does not match the corresponding word in the original text but should have been corrected.
(2) \textbf{Time taken for Spelling Correction } The time for correcting the text is also noted for benchmarking correction of large datasets.

%\end{description}

\noindent \textbf{N-Word Grams Spelling Correction Evaluation(SCE) Algorithm}
To make the correspondence between corrected and original OCR text, a window of N-word grams in the original newspaper text is considered which can be seen in a diagrammatic representation in Figure~\ref{figure:2}.
\begin{figure} [!htb]
\centering
\includegraphics[scale=0.8]{ngram}
\caption{Schematic diagram for alignment of spell corrected article text with original article text for a word $W_{k}$}
\label{figure:2}
\end{figure}
\noindent For each token in the spell corrected text, the corresponding token in the original text article along with 2 tokens before and 2 tokens after it are considered for alignment\footnote{ The choice of N=2 is based on the Word Split and Join errors in the dataset. This value can be set appropriately by considering the maximum difference of lengths in each line of OCR and original text in the dataset.}.
If the token being considered matches with any word in the original text article word's window and its spelling has been corrected when compared to the corresponding token in raw OCR text, then it is marked as a ``True Positive" which is actually rewarding the Spell corrector for making the correct spelling change. A ``False Positive" is marked if it does not match any of the words despite its spelling being corrected. 
%If the token being considered matches any of the words in the words window but no spelling correction has been made for it, then it is marked as a ``True Negative" and if it does not match any word in the window and the spelling corrector also did not correct it, then it is marked as a ``False Negative" as the word got missed by the corrector. 
%Several cases could occur during alignment like difference in the lengths of line text between OCR and Original text or while considering the first, second or the last tokens from the Corrected line text for which the corresponding word window in Original text needs to be smaller. All such cases have been covered in SCE Algorithm 1 which calls function `MatchWordGrams' (Algorithm2) for these different cases. 
%%%%DOUBT---USE THIS TABLE INSTEAD OF ALGORITHMS 1 AND 2???
Table~\ref{table1} describes the the process of marking each token in the corrected text as a TP, TN, FP or FN in each text article for calculation of accuracy. The final values of TP, TN, FP and FN are accumulated throughout the dataset to calculate accuracy.
 
\begin{table}[ h]
\centering
\caption{Calculation of accuracy in SCE algorithm}
\label{table1}
\begin{tabular}{|l|l|l|l|l|}
\hline
\backslashbox{Criteria to be checked}{Evaluation Metric}          & TP  & TN  & FP  & FN \\ \hline
Match found in N-word window? & Yes & Yes & No  & No \\ \hline
Spell correction was done?  & Yes & No  & Yes & No \\ \hline
\end{tabular}
\end{table}

\noindent Several scenarios could arise during the word alignment process due to difference in the lengths of text between OCR and original text. All such cases are depicted in Table~\ref{table2} which describes the window size of tokens to match in the original text (from j=starting index to ending index) for every token \textit{i} of the corrected text.

\begin{table}[h]
\centering
\caption{Different cases for word alignment}
\label{table2}
\scalebox{0.9}{
\begin{tabular}{|l|l|l|}
\hline
\backslashbox{Token index of CorrectedLine(i)}{Token index of OriginalLine}                                                   & Starting index (j)         & Ending index (j)         \\ \hline
Length{[}CorrectedLine{]} \textless4 or Length{[}OriginalLine{]}\textless4                                 & 0                          & Length{[}OriginalLine{]} \\ \hline
i=0                                                                                                        & 0                          & 3                        \\ \hline
i=1                                                                                                        & 0                          & 4                        \\ \hline
i=Length{[}CorrectedLine]-2 	&i-2			&Length{[}OriginalLine{]} \\ \hline
i=Length{[}CorrectedLine{]}-1	 & i-2                        & Length{[}OriginalLine{]} \\ \hline
i=Length{[}CorrectedLine{]}	 & i-2                        & Length{[}OriginalLine{]} \\ \hline
i=Length{[}CorrectedLine+1{]} & i-2                        & Length{[}OriginalLine{]} \\ \hline
i\textgreater=Length{[}CorrectedLine{]}+2                                                                  & Length{[}OriginalLine{]}-3 & Length{[}OriginalLine{]} \\ \hline
Any other value of i                                                                                            & i-2                        & i+3                      \\ \hline
\end{tabular}}
\end{table}

\noindent A limitation of the SCE algorithm is that it requires all 3 versions of a newspaper article (Original, Corrected and OCR) to have the same number of lines as alignment of line texts is performed. In case of difference in the number of lines of text due to some Word Split and Join errors, the word's window needs to be extended so as to cover previous and next line texts also for alignment.


%\begin{algorithm}[!h]
%\caption{SCE Algorithm for Spell Correction}
%  \KwIn{Ocr.txt,Corrected.txt,Original.txt}
%  \KwOut{Spell Corrector Accuracy }
%\SetKwFunction{MatchWordGrams}{MatchWordGrams}%
% $OcrLine$:=a line of text from Ocr.txt\;
% $CorrectedLine$:=a line of text from Corrected.txt\;
% $OriginalLine$:=a line of text from Original.txt\;
% $tp \leftarrow $0  $fp \leftarrow $0 $tn \leftarrow $0 $fn\leftarrow $0\;  
%	\For{(int i=0; i $<$ CorrectedLine.length ; i++) }
%	{
%
%    \If{(CorrectedLine.length$<$4 $||$ OriginalLine.length$<$4)}
%	{		
%    	\MatchWordGrams{(OcrLine,CorrectedLine,OriginalLine,0,OriginalLine.length,i)}\; 
%	}
%    \Else{
%   \If {(i==0)}
%   {
%\MatchWordGrams{(OcrLine,CorrectedLine,OriginalLine,0,3,0)}\;
%   }
%   \ElseIf{ (i==1)}
%   {
%\MatchWordGrams{(OcrLine,CorrectedLine,OriginalLine,0,4,1)}\;
%   }
%	\ElseIf{(i==(CorrectedLine.length-2) $||$ (CorrectedLine.length-1) $||$ (CorrectedLine.length) $||$ (CorrectedLine.length+1))}
%	{
%\MatchWordGrams{(OcrLine,CorrectedLine,OriginalLine,i-2,OriginalLine.length,i)}\;
%	}  
%	\ElseIf{(i $>$= CorrectedLine.length+2)}
%	{	
%\MatchWordGrams{(OcrLine,CorrectedLine,OriginalLine,OriginalLine.length-3,OriginalLine.length,i)}\;	
%	}
%	\Else
%	 {
%\MatchWordGrams{(OcrLine,CorrectedLine,OriginalLine,i-2,i+2,i)}\;	
%	}	
%   }
% }
% 	 $Accuracy=(tp+tn)/(tp+tn+fp+fn);$\
%\end{algorithm}
%
%
%
%
%\begin{algorithm}[H]
%\caption{MatchWordGrams Function called by Algorithm 1}
%\begin{algorithmic}
%\Function {MatchWordGrams}{OcrLine, CorrectedLine, OriginalLine, jstart, jend, i}
%  
% \For{(int j=jstart; j$<$jend; j++)}
%  {
%    \If{ ((CorrectedLine[i].equals(OriginalLine[j]))\&\&(!(OcrLine[i].equals(CorrectedLine[i]))))}
%     {
%	  $tp=tp+1$\;
%	  flag0=false\;
%	 \Return $tp$\;
%	  }
%	\ElseIf{((CorrectedLine[i].equals(OriginalLine[j]))\&\&(OcrLine[i].equals(CorrectedLine[i])))}
%	      {
%		 $tn=tn+1$\;
%		  flag1=false\;
%		\Return $tn$\;
%	      }
%}
%
%	 \If{(!(OcrLine[i].equals(CorrectedLine[i]))\&\&flag0==true)}
%	 {
%		    $fp=fp+1$\;
%		   \Return $fp$\;
%            }
%	 
%	 \ElseIf{((OcrLine[i].equals(CorrectedLine[i])) \&\& flag1==true)}
%	 {
%		    $fn=fn+1$\;
%		   \Return $fn$\;
%	 }
%\EndFunction
%\end{algorithmic}
%\end{algorithm}





\textbf{An Illustrative Example } The execution of the SCE algorithm can be demonstrated with the help of the following example:
Consider 3 versions of a scanned image of a newspaper article -- the original text of the scanned image, the raw OCR text and the text after spell correction. %in Figure~\ref{figure:4}. 
Assume, the texts are:

OcrLine= \textit{by tltn rejmrt of th cepert aocountauts who}

CorrectedLine= \textit{by than report of the expert accountants who}

OriginalLine= \textit{by the report of the expert accountants who} 

\noindent Here, for each token of CorrectedLine, we find its index and call the MatchWordGrams function accordingly. For the first token `by' at index i=0 in CorrectedLine, we consider the word window to be ``by the report" (index j=0 to 2) in OriginalLine by matching iteratively with each token to see if there is a match and also if there has been a spelling correction by comparing with the corresponding token in OcrLine. Here, no change was made to the spelling of `by' and it matches with a word in words window, so it is marked as a FN. For the second token `than' at index i=1, we consider the word window to be ``by the report of" (index j=0 to 3) for which there is no match in the window but there has been a spelling correction from `tltn' to `than', which implies the correction was wrong and the token is marked as a $FP$. For the third token `report' at index i=2, we consider the window as ``by the report of the" (index j=0 to 4) in Original Line and find that there is a match in the word window and there has been a spelling correction too from `rejmrt' to `report' which makes this token a $TP$. Similarly, rest of the tokens get marked for each line in the Corrected.txt. 

Another example can be considered from Line 10 in Figure~\ref{figure:3} and Figure~\ref{figure:4} where the number of tokens is different in CorrectedLine and OriginalLine. In such a case, direct alignment between tokens is not possible because of which the words window becomes useful. Here, when the last token `Richmond' of CorrectedLine is considered at index i=3, the corresponding words window becomes ``Jury now sitting at Richmond" (index j=1 to 5) for which there is a match in the words windows and corresponding spelling has also been changed from `tilchmond' to `Richmond' which makes it a $TP$. Had the word window not been considered, the corresponding token at index j=3 in OriginalLine would have been chosen as `sitting' which would have resulted in a $FP$. 
   

\begin{figure}[!htb]
\begin{center}
\includegraphics[scale=0.4]{img3}
\includegraphics[scale=0.75]{originalimg3}
\caption{Scanned image of a newspaper article (left) along with its original text (right)}
\label{figure:3}
\end{center}
\end{figure}


\begin{figure}[!htb]
\includegraphics[scale=0.75]{ocr3}
\includegraphics[scale=0.75]{corrected3}
\caption{OCR raw text (left) and Spell corrected text (right) of the article}
\label{figure:4}
\end{figure} 

\section{Empirical Evaluation and Results}
\label{spell:results}


\noindent \textbf{Aim: }The aim of our experiments is to answer the following research questions:

RQ1: Which spelling correction algorithm is better? %: auc score

RQ2: How does the variation of the parameters of SCE affects the result? %mention LCS here.

RQ3: How do parameters of the spelling correction algorithms affect the evaluation results?


\noindent \textbf{Materials: }The spelling correction algorithms are used to correct all the 300 OCR raw text articles in the dataset. The dictionary used for look-up is a concatenation of several public domain books from Project Gutenberg and lists of most frequent words from Wiktionary and the British National Corpus\footnote{http://norvig.com/big.txt}. This is augmented with a large people names list which is obtained  by running Stanford NER-CRF parser on subsets of the ClueWeb12 dataset made available in the TREC 2013 Crowdsourcing Track\footnote{http://boston.lti.cs.cmu.edu/clueweb12/TRECcrowdsourcing2013/}. This enhanced dictionary has been used to give special consideration to correction of person names in the dataset.
All our experiments are run on a 4GB RAM machine on Windows platform and the complete implementation of Edit distance and SCE algorithms has been carried out in Java. The context sensitive based spell correction algorithm from LingPipe 4.1.0 \footnote{http://alias-i.com/lingpipe/} version also implemented in Java has been used in our experiments. 

%%%%%%%Provide links to source code later

//////Mention answer to research questions here

\noindent \textbf{Methods: }In order to answer RQ1 for comparing the performance of spell correction algorithms, we use AUC score as a metric as described in previous section. For answering RQ2, we vary the value of the word window N=2 in the N-gram SCE algorithm and compare it with the LCS (Lowest Common Substring) algorithm as well to observe the changes in results of evaluation.
For addressing RQ3, we modify the parameters of both spell correction algorithms,i.e., value of edit distance in the edit distance algorithm and other parameters in context sensitive algorithm and study the modified spell correction evaluation results.
 

\noindent \textbf{Results: }

////////DOUBT: Should I mention the time taken to run spell correction algo as a parameter in both the algorithms?? 

For N-gram SCE algorithm having N=2, the edit distance based spell corrector shows an AUC score of 0.490 when corrected text is compared to OCR text and original article text using our SCE algorithm. We believe that the results are less accurate due to the presence of a large number of non-word, new line, word split and join errors in the OCR data which can not be corrected by the edit distance spelling corrector used for this research. 
On the other hand, the Lingpipe based context sensitive spelling corrector has a better AUC score of 0.554 demonstrating that in case of OCR text, this algorithm performs better.

We experimented with N=3 for the N-gram based SCE algorithm for which following results were obtained:



\begin{table}[]
\centering
\caption{AUC Scores when N=3 for SCE algorithm}
\label{my-label}
\begin{tabular}{lllll}
\cline{1-3}
\multicolumn{1}{|l|}{ED=2} & \multicolumn{1}{l|}{ED=3} & \multicolumn{1}{l|}{LP} &  &  \\ \cline{1-3}
\multicolumn{1}{|l|}{0.491} & \multicolumn{1}{l|}{0.493} & \multicolumn{1}{l|}{0.561} &  &  \\ \cline{1-3}
 &  &  &  &  \\
 &  &  &  & 
\end{tabular}
\end{table}



We  also compared our N-gram based SCE algorithm with the LCS (Longest Common Subsequence) algorithm\footnote{https://en.wikipedia.org/wiki/Longest\_common\_subsequence\_problem}. The LCS of corrected and original text gives a list of matching corrected words found in the original text. Following the similar evaluation procedure of calculating accuracy as in the N-word gram approach, if a corrected word finds a match in the LCS and its spelling is found to be corrected, then it is marked as a TP otherwise a TN and if no match is found in LCS and spelling has been corrected, then it is marked a FP or else FN. It was found that there is no statistically significant difference in accuracy when using either of the two algorithms. We posit that LCS is a special case of the N-word gram algorithm when the window size N is set to the total number of words in a linetext.

Our experiments on changing the parameters of spell correction algorithms used in this study reveal no significant changes in accuracy. For Edit distance algorithm, when edit distance is set to 3, the AUC score obtained in 0.492 which is not very high compared to the score obtained for an edit distance of 2.
For Context sentitive spell correction algorithm, following parameters were modified:


\section{Discussion}
\label{spell:discuss}
The edit distance based spell corrector used in this work corrects non-real word errors by focusing on isolated words in the dataset.
We believe a better accuracy of spell correction can be obtained by correcting the new line errors in the articles. This can be done by checking for if the word at last index of a linetext or the word at first index of the next linetext is a word not present in the dictionary and combining the two and checking again in the dictionary for a valid word. The new word, if present in the dictionary can be replaced by the two words from which it is formed thereby removing the New Line error. Similar approach can be applied for word split and join errors but would require each word of an article not present in the dictionary to be analyzed along with some window of words before and after it to make a correction. 
Since edit distance algorithm is governed by the dictionary choice, using a dictionary with historical terms, places and people names can also help perform spelling correction better and improve its accuracy.
%%DELETE THE PART ABOUT HOW NEW LINE AND WORD SPLIT AND JOIN ERRORS CAN BE REMOVED??
The LingPipe based context sensitive spell correction algorithm also employs a corpus dictionary for indexing the terms which suggests that the dictionary choice affects the results of spell correction in this case as well.


\section{Conclusion and Future Work}
\label{spell:fw}
In this paper, we presented a novel approach for automatic performance evaluation of spell correction on noisy OCR text through N-word grams alignment of the OCR, corrected and manually cleaned text. Preliminary results of application of our algorithm on an Edit distance based spell corrector evaluate its AUC score to be 0.49 while it evaluates to 0.56 for Context Sensitive based spell corrector. We believe the N-word gram SCE algorithm can be used to evaluate any kind of spell correction algorithm and provide a detailed performance by considering complete text coverage compared to simpler measures like percentage of corrected words or word error rate used previously. It can be further used to examine multiple spell correction algorithms and analyze the best spell correction algorithm suited for OCR data mining tasks.
In future, we plan to add more evaluation metrics to our evaluation algorithm. 
%%DELETE FUTURE WORK IF NOT REQUIRED??


\section{Acknowledgement}
This work has been supported by the National Endowment of Humanities Grant, NEH HD-51153-10.




%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
  \bibliographystyle{elsarticle-num} 
 \bibliography{aayushee}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

%\begin{thebibliography}{00}

%% \bibitem{label}
%% Text of bibliographic item

%\bibitem{}

%\end{thebibliography}

\end{document}
\endinput
%%
%% End of file `elsarticle-template-num.tex'.
